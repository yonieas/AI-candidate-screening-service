# AI-Powered Candidate Screening Service

This project is a backend service designed to automate the initial screening of job applications, as per the case study brief. It uses **FastAPI** for the web framework, Google's **Gemini Pro** as the LLM, and simulates a RAG pipeline to provide a structured evaluation of a candidate's CV and project report.

---

## Design Choices & Architecture

### 1. Framework: FastAPI

I chose FastAPI over Flask for several key reasons:

- **Native Asynchronous Support:** The requirement for a non-blocking `/evaluate` endpoint is a core feature of FastAPI. It allows for long-running AI tasks to be executed in the background without tying up server resources, which is critical for a responsive system.
- **Data Validation with Pydantic:** FastAPI's use of Pydantic models ensures that all incoming API requests are automatically validated. This reduces boilerplate code and prevents invalid data from entering the system.
- **Automatic API Documentation:** The interactive API docs (Swagger UI) generated by FastAPI are invaluable for development, testing, and eventual integration with a frontend or other services.
- **Performance:** FastAPI's high performance ensures the service can handle a significant number of requests efficiently.

### 2. OOP and Service-Oriented Design

I structured the core logic using an `AIEvaluationService` class. This Object-Oriented approach offers several benefits:

- **Scalability & Maintainability:** Encapsulating the AI logic within a dedicated service makes the code cleaner, easier to test, and simpler to maintain. If we wanted to add another evaluation type (e.g., a code sample), we could add a new method to this service without cluttering the API endpoint logic.
- **Separation of Concerns:** The FastAPI endpoints in `main.py` are only responsible for handling HTTP requests and responses. They delegate the complex business logic to the `AIEvaluationService`, which is a core principle of good software design.

### 3. Vector DB: ChromaDB (Simulated)

The case study requires using a vector database. While the provided code simulates this with a simple dictionary for demonstration purposes, a real implementation would use ChromaDB.

- **Why ChromaDB?** It is lightweight, open-source, and easy to set up and run locally, making it perfect for this type of project. It handles the embedding, storage, and retrieval of document chunks, which is the foundation of the RAG pipeline.
- **Ingestion Script (`ingest.py`):** A separate script would be used to process the "ground truth" documents (Job Description, Rubrics, etc.). This script would parse the PDFs, split them into manageable chunks, generate embeddings (using a model like `text-embedding-004`), and store them in a ChromaDB collection. This pre-processing step is done once, creating a knowledge base for the evaluation service to query against.

### 4. LLM Provider: Google Gemini

I opted for the free tier of the Gemini API as specified.

- **Seamless Integration:** The `google-generativeai` Python SDK is straightforward to use, especially for asynchronous operations (`generate_content_async`), which fits perfectly with FastAPI's async nature.
- **Prompt Chaining:** The `AIEvaluationService` implements a multi-step LLM chain. Instead of one massive prompt, I break down the task into smaller, focused prompts (CV evaluation, Project evaluation, Final Summary). This improves the accuracy and reliability of the LLM's output, as each step has a clear context and a simple task.

### 5. Asynchronous Task Handling

For the long-running evaluation process, I used FastAPI's built-in `BackgroundTasks`.

- **Why `BackgroundTasks`?** It is the simplest way to handle fire-and-forget tasks that don't need complex management. For this case study, it perfectly fulfills the requirement of immediately returning a job ID from the `/evaluate` endpoint while the AI processing happens in the background.
- **Production Alternative:** In a production environment, I would replace the in-memory jobs dictionary and `BackgroundTasks` with a more robust solution like **Celery** and **Redis**. Celery provides a dedicated task queue with support for retries, scheduling, and monitoring, while Redis would serve as the message broker and results backend, ensuring persistence and scalability.

---

## Setup and Running the Application

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd <your-repo-name>
```

### 2. Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Set Up Environment Variables

- Create a file named `.env` in the root directory.
- Open the `.env` file and add your Gemini API key:

```env
GEMINI_API_KEY="YOUR_API_KEY"
```

### 5. Run the FastAPI Server

```bash
uvicorn main:app --reload
```

The `--reload` flag automatically restarts the server when you make changes to the code.

### 6. Access the API

- The service will be running at [http://127.0.0.1:8000](http://127.0.0.1:8000).
- Access the interactive API documentation at [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs). From there, you can test all the endpoints directly in your browser.